{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"\"\"Load Native Data.\n",
    "\n",
    "* @File    :   1_load_native_data.ipynb\n",
    "* @Time    :   2025/03/26 09:58:38\n",
    "* @Author  :   Marc Ballestero Rib√≥\n",
    "* @Version :   0\n",
    "* @Contact :   marcballesteroribo@gmail.com\n",
    "* @License :   MIT\n",
    "* @Desc    :   None\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Native Data\n",
    "\n",
    "This notebook, part of the ETL phase of the project, is devoted to loading the HateXplain dataset and preparing it for the next preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import json\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path of the project's root directory\n",
    "ROOT_DIR = Path.resolve(Path.cwd() / \"../\")\n",
    "\n",
    "# Add root directory to sys.path\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "from src.utils.set_seed import set_seed\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "rng = set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory management\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "NATIVE_DATA_DIR = DATA_DIR / \"native\"\n",
    "PREPROCESSED_DATA_DIR = DATA_DIR / \"preprocessed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the native dataset\n",
    "with Path.open(NATIVE_DATA_DIR / \"dataset.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    dataset = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset contains the following fields:**\n",
    "- `post_id`: the unique identifier (UID) or primary key of the data instance.\n",
    "- `annotators`: The list of annotations from each annotator\n",
    "    - `annotators[label]` : The label assigned by the annotator to this post. Possible values: `[Hatespeech, Offensive, Normal]`-\n",
    "    - `annotators[annotator_id]` : The UID assigned to each annotator\n",
    "    - `annotators[target]` : A list of target community present in the post\n",
    "- `rationales`: A list of rationales selected by annotators. Each rationale represents a list with values 0 or 1. A value of 1 indicates that the token is part of the rationale selected by the annotator. The corresponding token can be retrieved using the same index position in `post_tokens`.\n",
    "- `post_tokens` : The list of tokens representing the post which was annotated.\n",
    "\n",
    "We will create a table with the following columns:\n",
    "- `post_id`: the UID of the data instance.\n",
    "- `labels`: a list of the labels assigned by the independent annotators to each data instance.\n",
    "- `targets`: a list of the target communities assigned by each independent annotator to each data instance.\n",
    "- `tokens`: the list of tokens corresponding to each data instance.\n",
    "- `rationales`: a list of the rationales selected by annotators to each data instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns from the JSON dataset\n",
    "post_ids = dataset.keys()\n",
    "\n",
    "annotations = [post[\"annotators\"] for post in dataset.values()]\n",
    "\n",
    "labels = [\n",
    "    [annotator[\"label\"] for annotator in annotation] for annotation in annotations\n",
    "]\n",
    "targets = [\n",
    "    [annotator[\"target\"] for annotator in annotation] for annotation in annotations\n",
    "]\n",
    "\n",
    "tokens = [post[\"post_tokens\"] for post in dataset.values()]\n",
    "rationales = [post[\"rationales\"] for post in dataset.values()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a Polars DataFrame with the data\n",
    "df_dataset = pl.DataFrame(\n",
    "    {\n",
    "        \"post_id\": post_ids,\n",
    "        \"labels\": labels,\n",
    "        \"targets\": targets,\n",
    "        \"tokens\": tokens,\n",
    "        \"rationales\": rationales,\n",
    "    },\n",
    "    schema={\n",
    "        \"post_id\": pl.String,\n",
    "        \"labels\": pl.List(pl.String),\n",
    "        \"targets\": pl.List(pl.List(pl.String)),\n",
    "        \"tokens\": pl.List(pl.String),\n",
    "        \"rationales\": pl.List(pl.List(pl.Float32)),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Save the DataFrame to disk in Parquet format\n",
    "df_dataset.write_parquet(PREPROCESSED_DATA_DIR / \"dataset_native.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesisdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
