{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"\"\"Explanations.\n",
    "\n",
    "* @File    :   7_explanations.ipynb\n",
    "* @Time    :   2025/04/01 19:23:50\n",
    "* @Author  :   Marc Ballestero Rib√≥\n",
    "* @Version :   0\n",
    "* @Contact :   marcballesteroribo@gmail.com\n",
    "* @License :   MIT\n",
    "* @Desc    :   None\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Explanations\n",
    "\n",
    "This notebook, part of the analysis phase of the project, is devoted to generating explanations using Integrated Gradients (IG), Integrated Directional Gradients (IDG) and Shapley Additive Explanations (SHAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:54:52.832820: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 18:54:52.900335: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-04 18:54:52.912977: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-04 18:54:52.920707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-04 18:54:52.990917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import json\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import shap\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "\n",
    "# Get the absolute path of the project's root directory\n",
    "ROOT_DIR = Path.resolve(Path.cwd() / \"../\")\n",
    "\n",
    "# Add root directory to sys.path\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "from src.utils.set_seed import set_seed\n",
    "\n",
    "from src.integrated_directional_gradients.IDG.calculate_gradients import execute_IDG\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "rng = set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory management\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "SPLITTED_DATA_DIR = DATA_DIR / \"splitted\"\n",
    "\n",
    "MODELS_DIR = ROOT_DIR / \"models\"\n",
    "\n",
    "OUTPUT_DIR = ROOT_DIR / \"output\"\n",
    "TREES_DIR = OUTPUT_DIR / \"constituency_trees\"\n",
    "\n",
    "IG_DIR = OUTPUT_DIR / \"integrated_gradients\"\n",
    "IDG_DIR = OUTPUT_DIR / \"integrated_directional_gradients\"\n",
    "SHAP_DIR = OUTPUT_DIR / \"shap\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and process the HateXplain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_test_trees = pl.read_parquet(TREES_DIR / \"test_2_classes_with_trees.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_clf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODELS_DIR / \"bert-base-uncased_2_classes\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODELS_DIR / \"bert-base-uncased_2_classes\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text column from the tokens\n",
    "df_test_trees = df_test_trees.with_columns(\n",
    "    pl.col(\"tokens\").map_elements(\n",
    "        lambda tokens: \" \".join(tokens),\n",
    "        return_dtype=pl.String,\n",
    "    ).alias(\"text\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Integrated Gradients\n",
    "\n",
    "Here, we apply the integrated gradients method to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_clf.to(device)\n",
    "\n",
    "cls_explainer = SequenceClassificationExplainer(model_clf, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IG:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1319/1376 [09:22<00:33,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 24796291_gab: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 111.19 MiB is free. Including non-PyTorch memory, this process has 7.53 GiB memory in use. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 407.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IG: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1375/1376 [09:52<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Execute IG\n",
    "with tqdm(total=len(df_test_trees), desc=\"Executing IG\") as pbar:\n",
    "    for post in df_test_trees.iter_rows(named=True):\n",
    "        text = post[\"text\"]\n",
    "        gt_cls = post[\"label\"]\n",
    "        guid = post[\"post_id\"]\n",
    "\n",
    "        # Execute IG\n",
    "        try:\n",
    "            attributions = cls_explainer(\n",
    "                text,\n",
    "                class_name=\"hatespeech\",\n",
    "                n_steps=150,\n",
    "            )\n",
    "            data = {\n",
    "                \"guid\": guid,\n",
    "                \"gt_cls\": gt_cls,\n",
    "                \"attributions\": attributions,\n",
    "            }\n",
    "\n",
    "            with Path.open(IG_DIR / f\"{guid}_ig_expl_data.json\", \"w\") as  f:\n",
    "                json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post {guid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Integrated Directional Gradients\n",
    "\n",
    "Here, we apply the integrated directional gradients method to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  19%|‚ñà‚ñâ        | 267/1376 [21:28<1:36:36,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 10040607_gab: '\\ufeff'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  23%|‚ñà‚ñà‚ñé       | 322/1376 [25:49<1:50:49,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 14445063_gab: '‚úùÔ∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 677/1376 [1:03:01<50:56,  4.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1178921633031114752_twitter: unsupported format character ',' (0x2c) at index 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 718/1376 [1:06:54<45:05,  4.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1179048983420899328_twitter: '‚ôÇÔ∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 725/1376 [1:07:24<37:47,  3.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1179098097160470529_twitter: '‚ôÇÔ∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 985/1376 [1:30:15<25:39,  3.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1178785944238841857_twitter: '‚ôÇÔ∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1017/1376 [1:32:46<24:29,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1178750958718914561_twitter: '‚ôÇÔ∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1078/1376 [1:37:25<30:06,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 10362793_gab: 'u.s1.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1104/1376 [1:39:34<11:55,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1109087701960146945_twitter: 'Ô∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1132/1376 [1:41:27<16:18,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1163082929557450753_twitter: 'Ô∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1140/1376 [1:41:49<09:30,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1171521116570959872_twitter: 'Ô∏è'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1213/1376 [1:46:11<09:02,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing post 1258666596333170689_twitter: 'üá¨'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing IDG:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1364/1376 [2:00:00<01:03,  5.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Execute IDG\n",
    "model_clf.to(torch.device(\"cpu\"))\n",
    "with tqdm(total=len(df_test_trees), desc=\"Executing IDG\") as pbar:\n",
    "    for post in df_test_trees.iter_rows(named=True):\n",
    "        tree = [post[\"tree\"]]\n",
    "        gt_cls = post[\"label\"]\n",
    "        guid = post[\"post_id\"]\n",
    "\n",
    "        # We will compute attributions wrt the positive (hatespeech) class\n",
    "        target_cls = 1\n",
    "\n",
    "        # Execute IDG\n",
    "        try:\n",
    "            coalitions, value_func, dividend_dir, p_tree = execute_IDG(\n",
    "                tree,\n",
    "                model_clf,\n",
    "                tokenizer,\n",
    "                target_cls,\n",
    "                IDG_DIR,\n",
    "                guid,\n",
    "                bert=True,\n",
    "            )\n",
    "            data = {\n",
    "                \"guid\": guid,\n",
    "                \"gt_cls\": gt_cls,\n",
    "                \"coalitions\": coalitions,\n",
    "                \"value_func\": value_func,\n",
    "                \"dividend_dir\": dividend_dir,\n",
    "                \"p_tree\": p_tree,\n",
    "            }\n",
    "\n",
    "            with Path.open(IDG_DIR / f\"{guid}_idg_expl_data.json\", \"w\") as  f:\n",
    "                json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post {guid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Shapley Additive Explanations\n",
    "Here, we apply the SHAP method to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30831, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model back to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_clf.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "PartitionExplainer explainer: 1377it [20:37,  1.10it/s]                          \n"
     ]
    }
   ],
   "source": [
    "# Create a text classification pipeline\n",
    "pipe = TextClassificationPipeline(\n",
    "    model=model_clf,\n",
    "    tokenizer=tokenizer,\n",
    "    top_k=None,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "\n",
    "test_data = df_test_trees.select(\"text\").to_numpy().flatten().tolist()\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(pipe)\n",
    "shap_values = explainer(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328c83c7bf5446b1a9d529e45496d5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, explanation in tqdma(enumerate(shap_values), total=len(shap_values)):\n",
    "    attributions = explanation.values[:, 1]\n",
    "    guid = df_test_trees[idx][\"post_id\"].to_list()[0]\n",
    "    gt_cls = df_test_trees[idx][\"label\"].to_list()[0]\n",
    "    tokens = np.array(\n",
    "        tokenizer.tokenize(df_test_trees[idx][\"text\"].to_list()[0], add_special_tokens=True),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        assert len(attributions) == len(tokens)\n",
    "    except AssertionError:\n",
    "        print(f\"Length mismatch for post {guid}: {len(attributions)} != {len(tokens)}\")\n",
    "        continue\n",
    "\n",
    "    data = {\n",
    "        \"guid\": guid,\n",
    "        \"gt_cls\": gt_cls,\n",
    "        \"attributions\": list(zip(tokens, attributions)),\n",
    "    }\n",
    "\n",
    "    with Path.open(SHAP_DIR / f\"{guid}_shap_expl_data.json\", \"w\") as  f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesisdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
